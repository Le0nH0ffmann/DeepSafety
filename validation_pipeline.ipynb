{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39209 files belonging to 43 classes.\n",
      "Train Classes available:  ['0' '1' '10' '11' '12' '13' '14' '15' '16' '17' '18' '19' '2' '20' '21'\n",
      " '22' '23' '24' '25' '26' '27' '28' '29' '3' '30' '31' '32' '33' '34' '35'\n",
      " '36' '37' '38' '39' '4' '40' '41' '42' '5' '6' '7' '8' '9']\n",
      "Found 17 files belonging to 5 classes.\n",
      "Val Classes available:  ['0' '1' '10' '11' '12']\n",
      "Predictions:  [ 0  1  1  1  1 10 10 25 11 11 11 11 12 12 12 12 12]\n",
      "Ground truth:  [ 0  1  1  1  1 10 10 11 11 11 11 11 12 12 12 12 12]\n",
      "Passt nicht :(\n",
      "./safetyBatches/Batch_0/Batch_0/0/01279.png\n",
      "Accuracy:  0.9411765\n",
      "Class ID: 0 Class Name: Speed limit (20km/h)\n"
     ]
    }
   ],
   "source": [
    "# This is the seed for your validation pipeline. It will allow you to load a model and run it on data from a directory.\n",
    "\n",
    "# //////////////////////////////////////// Setup\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def main():\n",
    "    # //////////////////////////////////////// Load model\n",
    "    model_name = \"Model_V0_10E\"\n",
    "    import_path = \"./trained_models/{}\".format(model_name)\n",
    "    model = tf.keras.models.load_model(import_path)\n",
    "\n",
    "    # //////////////////////////////////////// Load data\n",
    "    # You will need to unzip the respective batch folders.\n",
    "    # Obviously Batch_0 is not sufficient for testing as you will soon find out.\n",
    "    val_data_root = \"./safetyBatches/Batch_0/Batch_0/\"\n",
    "    train_data_root = \"./GTSRB_dataset/Train/\"\n",
    "\n",
    "    batch_size = 32\n",
    "    img_height = 224\n",
    "    img_width = 224\n",
    "\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(train_data_root)\n",
    "    # Get information on your train classes\n",
    "    train_class_names = np.array(train_ds.class_names)\n",
    "    print(\"Train Classes available: \", train_class_names)\n",
    "\n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        val_data_root,\n",
    "        seed=123,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    # Get information on your val classes\n",
    "    class_names = np.array(test_ds.class_names)\n",
    "    print(\"Val Classes available: \", class_names)\n",
    "\n",
    "    # get the ground truth labels\n",
    "    test_labels = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "    # Mapping test labels to the folder names instead of the index\n",
    "    for i in range(0, len(test_labels)):\n",
    "        test_labels[i] = int(class_names[test_labels[i]])\n",
    "\n",
    "    # Remember that we had some preprocessing before our training this needs to be repeated here\n",
    "    # Preprocessing as the tensorflow hub models expect images as float inputs [0,1]\n",
    "    normalization_layer = tf.keras.layers.Rescaling(1.0 / 255)\n",
    "    test_ds = test_ds.map(\n",
    "        lambda x, y: (normalization_layer(x), y)\n",
    "    )  # Where x—images, y—labels.\n",
    "\n",
    "    # //////////////////////////////////////// Inference.\n",
    "    predictions = model.predict(test_ds)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Mapping the prediction class id based on the folder names\n",
    "    for i in range(0, len(predictions)):\n",
    "        predictions[i] = int(train_class_names[predictions[i]])\n",
    "\n",
    "    print(\"Predictions: \", predictions)\n",
    "    print(\"Ground truth: \", test_labels)\n",
    "\n",
    "    for picture in range(0, len(predictions)):\n",
    "        if(predictions[picture] != test_labels[picture]):\n",
    "            print(\"Passt nicht :(\")\n",
    "            a = 0\n",
    "            picture_path = val_data_root + str(a) + \"/\" + \"01279.png\"#+ {\"{}\"}.format(01279) + \".png\"\n",
    "            print(picture_path)\n",
    "    #        plt.imshow(picture_path)\n",
    "    #plt.show()\n",
    "\n",
    "    # //////////////////////////////////////// Let the validation begin\n",
    "    # Probably you will want to at least migrate these to another script or class when this grows..\n",
    "    def accuracy(predictions, test_labels):\n",
    "        metric = tf.keras.metrics.Accuracy()\n",
    "        metric.update_state(predictions, test_labels)\n",
    "        return metric.result().numpy()\n",
    "\n",
    "    print(\"Accuracy: \", accuracy(predictions, test_labels))\n",
    "\n",
    "    class_names_df = pd.read_csv(\"./signnames.csv\", index_col=\"ClassId\")\n",
    "    print(\n",
    "        f'Class ID: {predictions[0]} Class Name: {class_names_df[\"SignName\"][predictions[0]]}'\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # There is more and this should get you started: https://www.tensorflow.org/api_docs/python/tf/keras/metrics\n",
    "    # However it is not about how many metrics you crank out, it is about whether you find the meaningful ones and report on them.\n",
    "    # Think about a system on how to decide which metric to go for..\n",
    "\n",
    "    # You are looking for a great package to generate your reports, let me recommend https://plotly.com/dash/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
